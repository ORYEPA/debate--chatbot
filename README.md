## Debate Chatbot

Servicio API que mantiene conversaciones de debate. Define un **topic** en el primer mensaje (sin `conversation_id`) y, a partir de ah√≠, **se mantiene en la postura inicial** para convencer a la otra parte.

üìÑ **Documentaci√≥n interactiva del API (Swagger UI):** [https://debate-api.fly.dev/docs](https://debate-api.fly.dev/docs)

---

### üß± Arquitectura (alto nivel)

```
Cliente (curl / App / Postman)
        ‚îÇ
        ‚ñº
   FastAPI (app)
        ‚îÇ             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Servidor LLM (Ollama u otro) ‚îÇ
        ‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
  L√≥gica de conversaci√≥n (memoria corta, reglas)
```

* **/health**: estado del servicio y base LLM.
* **/ask** (POST): `{ conversation_id, message }` ‚Üí Responde y mantiene hist√≥rico breve (hasta `MAX_HISTORY_PAIRS`).

---

## 1) Requisitos previos

> Puedes ejecutar **sin Docker** (entorno local con Python) o **con Docker Compose** (recomendado para reproducibilidad).

### Sistema

* **Windows 10/11**, **macOS 12+** o **Linux**.
* **Conexi√≥n a Internet** si usar√°s un servidor LLM remoto (p. ej. `OLLAMA_BASE_URL`).

### Herramientas

* **make** (GNU Make).

  * Windows: instala **Git for Windows** (trae Git Bash) o `choco install make` / `winget install GnuWin32.Make`.
  * macOS: `xcode-select --install` o `brew install make`.
  * Linux (Debian/Ubuntu): `sudo apt-get install make`.
* **Docker** + **Docker Compose v2** (subcomando `docker compose`).

  * Windows/macOS: **Docker Desktop**.
  * Linux: `sudo apt-get install docker.io` y luego seguir la gu√≠a oficial para habilitar `docker compose`.
* **Python 3.11+** (solo para ejecuci√≥n local sin Docker).

  * Windows: `winget install Python.Python.3.11` o desde python.org.
  * macOS (brew): `brew install python@3.11`.
  * Linux: `sudo apt-get install python3 python3-venv python3-pip`.

> El **Makefile** verifica dependencias m√≠nimas y, si faltan, te imprime instrucciones.

---

## 2) Variables de entorno

Crea un archivo **`.env`** en la ra√≠z del repo (o usa variables del sistema). Ejemplo:

```env
# App
APP_NAME=debate-chatbot
APP_ENV=development            # development | production
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=info                 # debug | info | warning | error
ENABLE_CORS=true

# LLM / Ollama
OLLAMA_BASE_URL=http://ollama:11434  # Apunta a tu servidor Ollama o proveedor LLM
MODEL_NAME=llama3:8b
HTTP_TIMEOUT_SECONDS=60
KEEP_ALIVE=5m

# Conversaci√≥n
MAX_HISTORY_PAIRS=5
REPLY_CHAR_LIMIT=1200
NUM_PREDICT_CAP=350
NUM_CTX=8192

# Reglas universales (opcional; JSON o texto)
UNIVERSAL_RULES=Keep responses on topic and persuasive.
```

**Descripci√≥n breve:**

* `OLLAMA_BASE_URL`: URL base del servidor LLM (por ej., Ollama local: `http://localhost:11434`).
* `MODEL_NAME`: modelo a usar (p. ej. `llama3:8b`).
* `MAX_HISTORY_PAIRS`: pares (usuario/bot) recientes que se conservan.
* `REPLY_CHAR_LIMIT`, `NUM_PREDICT_CAP`, `NUM_CTX`: controles de tama√±o y contexto.
* `KEEP_ALIVE`, `HTTP_TIMEOUT_SECONDS`: par√°metros para timeouts/conexiones.

> Si usas **Ollama en Docker** con el `docker-compose.yml` provisto, deja `OLLAMA_BASE_URL=http://ollama:11434` para que el contenedor `api` resuelva el nombre de servicio `ollama`.

---

## 3) Ejecuci√≥n con Docker (recomendada)

### 3.1 Estructura esperada

```
.
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ main.py         # FastAPI
‚îÇ  ‚îú‚îÄ config.py       # Lee env vars
‚îÇ  ‚îú‚îÄ models/ ...
‚îÇ  ‚îî‚îÄ tests/          # Pytest
‚îú‚îÄ Dockerfile
‚îú‚îÄ docker-compose.yml
‚îú‚îÄ requirements.txt
‚îú‚îÄ Makefile
‚îú‚îÄ .env               # tus variables
‚îî‚îÄ README.md
```

### 3.2 `docker-compose.yml` (referencia)

> El repositorio ya debe incluirlo; en caso de necesitar uno m√≠nimo, este ejemplo funciona:

```yaml
version: "3.9"
services:
  api:
    build: .
    env_file: .env
    ports:
      - "${PORT:-8000}:8000"
    depends_on:
      - ollama
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
volumes:
  ollama:
```

> Para usar **modelo local**, entra al contenedor `ollama` y ejecuta `ollama pull llama3:8b`. El Makefile incluye un atajo.

### 3.3 Comandos principales (Makefile)

* **`make`** ‚Äî lista de comandos disponibles.
* **`make run`** ‚Äî levanta API + dependencias con Docker.
* **`make down`** ‚Äî apaga servicios.
* **`make clean`** ‚Äî teardown completo (vol√∫menes e im√°genes locales).

> Tambi√©n tienes `make install` (setup local) y `make test` (pytest).

---

## 4) Ejecuci√≥n local (sin Docker)

1. Crear entorno:

```bash
make install
```

> En Windows (PowerShell) manualmente:
>
> ```pwsh
> py -3 -m venv .venv
> .\.venv\Scripts\Activate.ps1
> python -m pip install -r requirements.txt
> ```

2. Exportar variables (o `.env`).

3. Correr servidor:

```bash
. .venv/bin/activate
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

---

## 5) Endpoints y pruebas r√°pidas

### Salud

```bash
curl -s http://localhost:8000/health
```

### Conversaci√≥n

```bash
# Primer mensaje: define topic y postura
curl -s -X POST http://localhost:8000/ask \
  -H 'Content-Type: application/json' \
  -d '{"conversation_id": null, "message": "Hola, ¬øde qu√© vamos a debatir?"}' | jq

# Respuesta subsiguiente: reutiliza conversation_id devuelto
curl -s -X POST http://localhost:8000/ask \
  -H 'Content-Type: application/json' \
  -d '{"conversation_id": "<id devuelto>", "message": "No estoy de acuerdo por X raz√≥n"}' | jq
```

### Tests

```bash
make test
```

---

## 6) Resoluci√≥n de problemas (FAQ)

* **`could_not_reach_ollama` / timeouts**: verifica `OLLAMA_BASE_URL` y que el servidor LLM est√© activo; si usas Docker, `make run` debe levantar `ollama`.
* **`docker compose` no existe**: usa `docker-compose` cl√°sico o actualiza Docker Desktop; el Makefile detecta ambas variantes.
* **Windows y `source`**: usa Git Bash para `make`; en PowerShell, activa el venv con `./.venv/Scripts/Activate.ps1`.
* **Modelo no encontrado**: entra al contenedor `ollama` y ejecuta `ollama pull <MODEL_NAME>`.

---

## 7) Est√°ndares de c√≥digo

* **Formateo**: puedes usar *black* y *ruff* (opcional).
* **Tests**: *pytest* en `app/tests`.

---

## 8) Seguridad y despliegue

* No expongas la API p√∫blica sin autenticaci√≥n si usas modelos locales.
* Valida l√≠mites (`REPLY_CHAR_LIMIT`, `NUM_PREDICT_CAP`).
* Logs: usa `LOG_LEVEL` adecuado en producci√≥n.

---

# Makefile

```makefile
# Makefile ‚Äî Debate Chatbot
# Usa: `make` para ver ayuda

SHELL := /bin/bash
PY := $(shell command -v python3 >/dev/null 2>&1 && echo python3 || (command -v py >/dev/null 2>&1 && echo 'py -3' || echo python3))
DC := $(shell docker compose version >/dev/null 2>&1 && echo 'docker compose' || (command -v docker-compose >/dev/null 2>&1 && echo 'docker-compose' || echo 'docker compose'))

.ONESHELL:
.DEFAULT_GOAL := help

## help: Muestra esta ayuda
help:
	@echo "Comandos disponibles:";
	@grep -E '^[a-zA-Z_-]+:.*?##' $(MAKEFILE_LIST) | sed -E 's/:.*##/: /' | sort | column -s: -t

## install: Instala requerimientos locales (venv + requirements.txt)
install: check-make
	@# Verificar Python
	@if ! command -v $(PY) >/dev/null 2>&1; then \
		echo "‚ùå Python 3.11+ no encontrado."; \
		echo "  Windows: winget install Python.Python.3.11"; \
		echo "  macOS:   brew install python@3.11"; \
		echo "  Linux:   sudo apt-get install python3 python3-venv python3-pip"; \
		exit 1; \
	fi
	@# Crear venv si no existe
	@if [ ! -d .venv ]; then $(PY) -m venv .venv; fi
	@# Activar venv y pip install (Git Bash / Unix shells)
	@source .venv/bin/activate 2>/dev/null || true; \
	python -m pip install --upgrade pip; \
	pip install -r requirements.txt
	@echo "‚úÖ Entorno instalado en .venv"

## test: Ejecuta tests con pytest
test:
	@source .venv/bin/activate 2>/dev/null || true; \
	pytest -q || { echo "‚ùå Pytest fall√≥ o no est√° instalado."; exit 1; }

## run: Levanta API (y servicios relacionados) en Docker
run: check-docker
	@echo "‚ñ∂ Levantando servicios con $(DC) ..."
	@$(DC) up -d --build
	@echo "‚Ñπ Si usas Ollama local, puedes cargar el modelo con: make pull-model"
	@echo "üåê API en: http://localhost:$${PORT:-8000}"

## pull-model: Pull del modelo en el contenedor de Ollama (usa MODEL_NAME de .env)
pull-model: check-docker
	@set -a; [ -f .env ] && source .env; set +a; \
	MODEL="$${MODEL_NAME:-llama3:8b}"; \
	CID=$$($(DC) ps -q ollama); \
	if [ -z "$$CID" ]; then echo "‚ùå Servicio 'ollama' no est√° corriendo"; exit 1; fi; \
	$(DC) exec ollama ollama pull $$MODEL

## down: Apaga los servicios Docker
down: check-docker
	@$(DC) down

## clean: Teardown completo (containers, vol√∫menes, im√°genes locales)
clean: check-docker
	@$(DC) down -v --rmi local --remove-orphans || true
	@echo "üßπ Limpieza completa"

## env: Crea .env desde .env.example si no existe
env:
	@if [ -f .env ]; then echo ".env ya existe"; exit 0; fi
	@if [ -f .env.example ]; then cp .env.example .env && echo "Creado .env desde .env.example"; else \
	 echo "No hay .env.example. Crea .env manualmente (ver README)."; fi

## check-docker: Verifica Docker y Compose
check-docker:
	@if ! command -v docker >/dev/null 2>&1; then \
		echo "‚ùå Docker no encontrado."; \
		echo "  Windows/macOS: Instala Docker Desktop."; \
		echo "  Linux: sudo apt-get install docker.io"; \
		exit 1; \
	fi
	@if docker compose version >/dev/null 2>&1; then \
		echo "‚úÖ docker compose OK"; \
	elif command -v docker-compose >/dev/null 2>&1; then \
		echo "‚úÖ docker-compose OK (modo legacy)"; \
	else \
		echo "‚ùå Docker Compose no encontrado. Actualiza Docker Desktop o instala docker-compose."; \
		exit 1; \
	fi

## check-make: Verifica que 'make' sea ejecutable
check-make:
	@if ! command -v make >/dev/null 2>&1; then \
		echo "‚ùå 'make' no encontrado."; \
		echo "  Windows: usa Git Bash (instala Git for Windows) o winget/choco para 'make'."; \
		echo "  macOS: xcode-select --install o brew install make"; \
		echo "  Linux: sudo apt-get install make"; \
		exit 1; \
	fi

.PHONY: help install test run down clean env check-docker check-make pull-model
```
